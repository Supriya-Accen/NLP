{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NLP\n",
    "\n",
    "# NLP BASICS(1ST LECTURE) and applications of NLP\n",
    "# # USEFUL LINKS:https://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1\n",
    "# # https://www.upgrad.com/blog/5-applications-of-natural-language-processing-for-businesses/\n",
    "\n",
    "# IMPORTANT TERMS USED:\n",
    "# # https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html\n",
    "\n",
    "\n",
    "# oTHER IMP TERMS:\n",
    "# lEXICON-set of words beloning to a particular domain\n",
    "# Polarity-+ve,-ve and neutral(-1,0 or 1)(views)\n",
    "# subjective-when an opinion is involved(delhi is great city)\n",
    "# objective-no opinion is involved(delhi is the capital=fact)\n",
    "\n",
    "# EMBEDDING: process of converting text to numbers and reverse\n",
    "# in NLP, embedding techinique used decides how good ur model is!\n",
    "\n",
    "# Work embedding techniques: text to numbe1\n",
    "# 1.Count Vectorizer\n",
    "# 2.TF IDF\n",
    "# first 2 mthods do not support modeling; they jus provide embedding \n",
    "\n",
    "# 3.Word2Ved\n",
    "# whereas this one supports both(hence invloves hyperparametr tuning as well)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/introduction-to-word-embeddings-4cf857b12edc\n",
    "# https://towardsdatascience.com/natural-language-processing-count-vectorization-with-scikit-learn-e7804269bb5e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\91808\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (3.4)\n",
      "Requirement already satisfied: six in c:\\users\\91808\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: singledispatch in c:\\users\\91808\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from nltk) (3.4.0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91808\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1=\"Mumbai is great in terms of food food and night life\"\n",
    "sent_2=\"Pune is great in terms of weather and stay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mumbai',\n",
       " 'is',\n",
       " 'great',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'food',\n",
       " 'and',\n",
       " 'night',\n",
       " 'life',\n",
       " 'Pune',\n",
       " 'is',\n",
       " 'great',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'weather',\n",
       " 'and',\n",
       " 'stay']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "words_list = word_tokenize(sent_1)+word_tokenize(sent_2)\n",
    "words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91808\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove=list(stopwords.words(\"english\"))+list(punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mumbai',\n",
       " 'great',\n",
       " 'terms',\n",
       " 'food',\n",
       " 'night',\n",
       " 'life',\n",
       " 'Pune',\n",
       " 'great',\n",
       " 'terms',\n",
       " 'weather',\n",
       " 'stay']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_words=[x for x in words_list if x not in remove]\n",
    "final_words\n",
    "#Cleaning of text-removed all punctuations and stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Mumbai',\n",
       " 'Pune',\n",
       " 'food',\n",
       " 'great',\n",
       " 'life',\n",
       " 'night',\n",
       " 'stay',\n",
       " 'terms',\n",
       " 'weather'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv=CountVectorizer(stop_words='english')\n",
    "cv\n",
    "#(Parameter study)lowercase : boolean, True by default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mumbai is great in terms of food food and night life',\n",
       " 'Pune is great in terms of weather and stay']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=[sent_1,sent_2]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect=cv.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['food',\n",
       " 'great',\n",
       " 'life',\n",
       " 'mumbai',\n",
       " 'night',\n",
       " 'pune',\n",
       " 'stay',\n",
       " 'terms',\n",
       " 'weather']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names()\n",
    "#observation-it removes stopwords and punctuations; in addition removes duplicates as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=vect.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 1, 1, 1, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1622776601683795\n"
     ]
    }
   ],
   "source": [
    "#you can find and distance between them\n",
    "from scipy.spatial.distance import euclidean, cdist, pdist\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cs\n",
    "\n",
    "print(euclidean(x[0], x[1])) #Euclidean distance-distance between 2 points;pythagoras\n",
    "\n",
    "\n",
    "#Manhattan distance. Definition: The distance between two points measured along axes at right angles. \n",
    "#In a plane with p1 at (x1, y1) and p2 at (x2, y2), it is |x1 - x2| + |y1 - y2|.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #conclusion:Count Vectoriser is a embedding method which converts text to number and works on \n",
    "# basis of frequency of words and has nothing to do with meaning of text;i.e not symantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF is a weighted model commonly used for information retrieval problems. \n",
    "# It aims to convert the text documents into vector models on the basis of occurrence of words in the documents \n",
    "# without taking considering the exact ordering. For Example – let say there is a dataset of N text documents, \n",
    "# In any document “D”, TF and IDF will be defined as –\n",
    "\n",
    "# Term Frequency (TF) – TF for a term “t” is defined as the count of a term “t” in a document “D”\n",
    "\n",
    "# Inverse Document Frequency (IDF) – IDF for a term is defined as logarithm of ratio of total documents available in \n",
    "# the corpus and number of documents containing the term T.\n",
    "\n",
    "#EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1=\"Mumbai is heart of Maharashta\"\n",
    "sent2=\"Mumbai is popular for its beaches\"\n",
    "sent3=\"The weather in Mumbai is hot and humid\"\n",
    "\n",
    "text2=[sent1,sent2,sent3]\n",
    "\n",
    "#idEALLY Mumbai has been repeated in all 3 docs hence it shudnt b of much importance to us.lets check:TF*IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf=TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect2=tf.fit_transform(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.65249088, 0.        , 0.        , 0.65249088,\n",
       "        0.38537163, 0.        , 0.        ],\n",
       "       [0.65249088, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.38537163, 0.65249088, 0.        ],\n",
       "       [0.        , 0.        , 0.54645401, 0.54645401, 0.        ,\n",
       "        0.32274454, 0.        , 0.54645401]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect2.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beaches',\n",
       " 'heart',\n",
       " 'hot',\n",
       " 'humid',\n",
       " 'maharashta',\n",
       " 'mumbai',\n",
       " 'popular',\n",
       " 'weather']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Conclusion: Even TF IDF works on principle of frequency and not meaning\n",
    "# However here instead of 0 and 1 we have values inbetween 0 and 1 which represents weightage of that word in sentence/document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
